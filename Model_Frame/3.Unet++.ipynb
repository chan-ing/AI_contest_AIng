{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "import torchvision.models as models\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import torch.nn.functional as F\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_decode(mask_rle, shape):\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout2d(p=0.2),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetpp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNetpp, self).__init__()\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.dconv_down = double_conv(3, 32)\n",
    "        \n",
    "        # ~ 0\n",
    "        self.dconv_down0_0 = double_conv(32, 32)\n",
    "        self.dconv_down1_0 = double_conv(32, 64)\n",
    "        self.dconv_down2_0 = double_conv(64, 128)\n",
    "        self.dconv_down3_0 = double_conv(128, 256)\n",
    "        self.dconv_down4_0 = double_conv(256, 512)\n",
    "\n",
    "        # ~ 1\n",
    "        self.dconv_down0_1 = double_conv(32+64, 32)\n",
    "        self.dconv_down1_1 = double_conv(64+128, 64)\n",
    "        self.dconv_down2_1 = double_conv(128+256, 128)\n",
    "        self.dconv_down3_1 = double_conv(256+512, 256)\n",
    "\n",
    "        #~ 2\n",
    "        self.dconv_down0_2 = double_conv(64+64, 32)\n",
    "        self.dconv_down1_2 = double_conv(128+128, 64)\n",
    "        self.dconv_down2_2 = double_conv(256+256, 128)\n",
    "\n",
    "        #~ 3\n",
    "        self.dconv_down0_3 = double_conv(96+64, 32)\n",
    "        self.dconv_down1_3 = double_conv(192+128, 64)\n",
    "\n",
    "        #~ 4\n",
    "        self.dconv_down0_4 = double_conv(128+64,32)\n",
    "        \n",
    "        self.output1 = nn.Conv2d(32, 1, 1)\n",
    "        self.output2 = nn.Conv2d(32, 1, 1)\n",
    "        self.output3 = nn.Conv2d(32, 1, 1)\n",
    "        self.output4 = nn.Conv2d(32, 1, 1)\n",
    "\n",
    "        self.Drop_out = nn.Dropout2d(0.2) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dconv_down(x)  #32,224,224\n",
    "\n",
    "        x0_0 = self.dconv_down0_0(x)   #32,224,224\n",
    "        x = self.maxpool(x0_0)          #32,112,112\n",
    "        x1_0 = self.dconv_down1_0(x)     #64,112,112\n",
    "        x = self.upsample(x1_0)\n",
    "        x = torch.cat([x0_0, self.upsample(x1_0)], dim=1)  #64+32,224,224\n",
    "        x0_1 = self.dconv_down0_1(x)   #32,224,224\n",
    "\n",
    "        x = self.maxpool(x1_0)  #64,56,56\n",
    "        x2_0 = self.dconv_down2_0(x)   #128,56,56\n",
    "        x = torch.cat([x1_0,self.upsample(x2_0)],dim=1)  #64+128,112,112\n",
    "        x1_1 = self.dconv_down1_1(x)  #64,112,112\n",
    "        x = torch.cat([x0_0,x0_1,self.upsample(x1_1)], dim=1) #32+32+64,224,224\n",
    "        x0_2 = self.dconv_down0_2(x)  #32,224,224\n",
    "        \n",
    "        x = self.maxpool(x2_0)\n",
    "        x3_0 = self.dconv_down3_0(x)\n",
    "        x = torch.cat([x2_0,self.upsample(x3_0)], dim=1)\n",
    "        x2_1 = self.dconv_down2_1(x)\n",
    "        x = torch.cat([x1_0,x1_1,self.upsample(x2_1)], dim=1)\n",
    "        x1_2 = self.dconv_down1_2(x)\n",
    "        x = torch.cat([x0_0, x0_1, x0_2, self.upsample(x1_2)], dim=1)\n",
    "        x0_3 = self.dconv_down0_3(x)\n",
    "\n",
    "        x = self.maxpool(x3_0)\n",
    "        x4_0 = self.dconv_down4_0(x)\n",
    "        x = torch.cat([x3_0,self.upsample(x4_0)], dim=1)\n",
    "        x3_1 = self.dconv_down3_1(x)\n",
    "        x = torch.cat([x2_0,x2_1,self.upsample(x3_1)], dim=1)\n",
    "        x2_2 = self.dconv_down2_2(x)\n",
    "        x = torch.cat([x1_0, x1_1, x1_2, self.upsample(x2_2)], dim=1)\n",
    "        x1_3 = self.dconv_down1_3(x)\n",
    "        x = torch.cat([x0_0, x0_1, x0_2,x0_3 ,self.upsample(x1_3)], dim=1)\n",
    "        x0_4 = self.dconv_down0_4(x)\n",
    "        \n",
    "        output1 = self.output1(x0_1)\n",
    "        output2 = self.output1(x0_2)\n",
    "        output3 = self.output1(x0_3)\n",
    "        output4 = self.output1(x0_4)\n",
    "\n",
    "        output = (output1 + output2 + output3 + output4)/4\n",
    "   \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 224, 224]             896\n",
      "       BatchNorm2d-2         [-1, 32, 224, 224]              64\n",
      "              ReLU-3         [-1, 32, 224, 224]               0\n",
      "         Dropout2d-4         [-1, 32, 224, 224]               0\n",
      "            Conv2d-5         [-1, 32, 224, 224]           9,248\n",
      "       BatchNorm2d-6         [-1, 32, 224, 224]              64\n",
      "              ReLU-7         [-1, 32, 224, 224]               0\n",
      "            Conv2d-8         [-1, 32, 224, 224]           9,248\n",
      "       BatchNorm2d-9         [-1, 32, 224, 224]              64\n",
      "             ReLU-10         [-1, 32, 224, 224]               0\n",
      "        Dropout2d-11         [-1, 32, 224, 224]               0\n",
      "           Conv2d-12         [-1, 32, 224, 224]           9,248\n",
      "      BatchNorm2d-13         [-1, 32, 224, 224]              64\n",
      "             ReLU-14         [-1, 32, 224, 224]               0\n",
      "        MaxPool2d-15         [-1, 32, 112, 112]               0\n",
      "           Conv2d-16         [-1, 64, 112, 112]          18,496\n",
      "      BatchNorm2d-17         [-1, 64, 112, 112]             128\n",
      "             ReLU-18         [-1, 64, 112, 112]               0\n",
      "        Dropout2d-19         [-1, 64, 112, 112]               0\n",
      "           Conv2d-20         [-1, 64, 112, 112]          36,928\n",
      "      BatchNorm2d-21         [-1, 64, 112, 112]             128\n",
      "             ReLU-22         [-1, 64, 112, 112]               0\n",
      "         Upsample-23         [-1, 64, 224, 224]               0\n",
      "         Upsample-24         [-1, 64, 224, 224]               0\n",
      "           Conv2d-25         [-1, 32, 224, 224]          27,680\n",
      "      BatchNorm2d-26         [-1, 32, 224, 224]              64\n",
      "             ReLU-27         [-1, 32, 224, 224]               0\n",
      "        Dropout2d-28         [-1, 32, 224, 224]               0\n",
      "           Conv2d-29         [-1, 32, 224, 224]           9,248\n",
      "      BatchNorm2d-30         [-1, 32, 224, 224]              64\n",
      "             ReLU-31         [-1, 32, 224, 224]               0\n",
      "        MaxPool2d-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 128, 56, 56]          73,856\n",
      "      BatchNorm2d-34          [-1, 128, 56, 56]             256\n",
      "             ReLU-35          [-1, 128, 56, 56]               0\n",
      "        Dropout2d-36          [-1, 128, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]         147,584\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "         Upsample-40        [-1, 128, 112, 112]               0\n",
      "           Conv2d-41         [-1, 64, 112, 112]         110,656\n",
      "      BatchNorm2d-42         [-1, 64, 112, 112]             128\n",
      "             ReLU-43         [-1, 64, 112, 112]               0\n",
      "        Dropout2d-44         [-1, 64, 112, 112]               0\n",
      "           Conv2d-45         [-1, 64, 112, 112]          36,928\n",
      "      BatchNorm2d-46         [-1, 64, 112, 112]             128\n",
      "             ReLU-47         [-1, 64, 112, 112]               0\n",
      "         Upsample-48         [-1, 64, 224, 224]               0\n",
      "           Conv2d-49         [-1, 32, 224, 224]          36,896\n",
      "      BatchNorm2d-50         [-1, 32, 224, 224]              64\n",
      "             ReLU-51         [-1, 32, 224, 224]               0\n",
      "        Dropout2d-52         [-1, 32, 224, 224]               0\n",
      "           Conv2d-53         [-1, 32, 224, 224]           9,248\n",
      "      BatchNorm2d-54         [-1, 32, 224, 224]              64\n",
      "             ReLU-55         [-1, 32, 224, 224]               0\n",
      "        MaxPool2d-56          [-1, 128, 28, 28]               0\n",
      "           Conv2d-57          [-1, 256, 28, 28]         295,168\n",
      "      BatchNorm2d-58          [-1, 256, 28, 28]             512\n",
      "             ReLU-59          [-1, 256, 28, 28]               0\n",
      "        Dropout2d-60          [-1, 256, 28, 28]               0\n",
      "           Conv2d-61          [-1, 256, 28, 28]         590,080\n",
      "      BatchNorm2d-62          [-1, 256, 28, 28]             512\n",
      "             ReLU-63          [-1, 256, 28, 28]               0\n",
      "         Upsample-64          [-1, 256, 56, 56]               0\n",
      "           Conv2d-65          [-1, 128, 56, 56]         442,496\n",
      "      BatchNorm2d-66          [-1, 128, 56, 56]             256\n",
      "             ReLU-67          [-1, 128, 56, 56]               0\n",
      "        Dropout2d-68          [-1, 128, 56, 56]               0\n",
      "           Conv2d-69          [-1, 128, 56, 56]         147,584\n",
      "      BatchNorm2d-70          [-1, 128, 56, 56]             256\n",
      "             ReLU-71          [-1, 128, 56, 56]               0\n",
      "         Upsample-72        [-1, 128, 112, 112]               0\n",
      "           Conv2d-73         [-1, 64, 112, 112]         147,520\n",
      "      BatchNorm2d-74         [-1, 64, 112, 112]             128\n",
      "             ReLU-75         [-1, 64, 112, 112]               0\n",
      "        Dropout2d-76         [-1, 64, 112, 112]               0\n",
      "           Conv2d-77         [-1, 64, 112, 112]          36,928\n",
      "      BatchNorm2d-78         [-1, 64, 112, 112]             128\n",
      "             ReLU-79         [-1, 64, 112, 112]               0\n",
      "         Upsample-80         [-1, 64, 224, 224]               0\n",
      "           Conv2d-81         [-1, 32, 224, 224]          46,112\n",
      "      BatchNorm2d-82         [-1, 32, 224, 224]              64\n",
      "             ReLU-83         [-1, 32, 224, 224]               0\n",
      "        Dropout2d-84         [-1, 32, 224, 224]               0\n",
      "           Conv2d-85         [-1, 32, 224, 224]           9,248\n",
      "      BatchNorm2d-86         [-1, 32, 224, 224]              64\n",
      "             ReLU-87         [-1, 32, 224, 224]               0\n",
      "        MaxPool2d-88          [-1, 256, 14, 14]               0\n",
      "           Conv2d-89          [-1, 512, 14, 14]       1,180,160\n",
      "      BatchNorm2d-90          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-91          [-1, 512, 14, 14]               0\n",
      "        Dropout2d-92          [-1, 512, 14, 14]               0\n",
      "           Conv2d-93          [-1, 512, 14, 14]       2,359,808\n",
      "      BatchNorm2d-94          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-95          [-1, 512, 14, 14]               0\n",
      "         Upsample-96          [-1, 512, 28, 28]               0\n",
      "           Conv2d-97          [-1, 256, 28, 28]       1,769,728\n",
      "      BatchNorm2d-98          [-1, 256, 28, 28]             512\n",
      "             ReLU-99          [-1, 256, 28, 28]               0\n",
      "       Dropout2d-100          [-1, 256, 28, 28]               0\n",
      "          Conv2d-101          [-1, 256, 28, 28]         590,080\n",
      "     BatchNorm2d-102          [-1, 256, 28, 28]             512\n",
      "            ReLU-103          [-1, 256, 28, 28]               0\n",
      "        Upsample-104          [-1, 256, 56, 56]               0\n",
      "          Conv2d-105          [-1, 128, 56, 56]         589,952\n",
      "     BatchNorm2d-106          [-1, 128, 56, 56]             256\n",
      "            ReLU-107          [-1, 128, 56, 56]               0\n",
      "       Dropout2d-108          [-1, 128, 56, 56]               0\n",
      "          Conv2d-109          [-1, 128, 56, 56]         147,584\n",
      "     BatchNorm2d-110          [-1, 128, 56, 56]             256\n",
      "            ReLU-111          [-1, 128, 56, 56]               0\n",
      "        Upsample-112        [-1, 128, 112, 112]               0\n",
      "          Conv2d-113         [-1, 64, 112, 112]         184,384\n",
      "     BatchNorm2d-114         [-1, 64, 112, 112]             128\n",
      "            ReLU-115         [-1, 64, 112, 112]               0\n",
      "       Dropout2d-116         [-1, 64, 112, 112]               0\n",
      "          Conv2d-117         [-1, 64, 112, 112]          36,928\n",
      "     BatchNorm2d-118         [-1, 64, 112, 112]             128\n",
      "            ReLU-119         [-1, 64, 112, 112]               0\n",
      "        Upsample-120         [-1, 64, 224, 224]               0\n",
      "          Conv2d-121         [-1, 32, 224, 224]          55,328\n",
      "     BatchNorm2d-122         [-1, 32, 224, 224]              64\n",
      "            ReLU-123         [-1, 32, 224, 224]               0\n",
      "       Dropout2d-124         [-1, 32, 224, 224]               0\n",
      "          Conv2d-125         [-1, 32, 224, 224]           9,248\n",
      "     BatchNorm2d-126         [-1, 32, 224, 224]              64\n",
      "            ReLU-127         [-1, 32, 224, 224]               0\n",
      "          Conv2d-128          [-1, 1, 224, 224]              33\n",
      "          Conv2d-129          [-1, 1, 224, 224]              33\n",
      "          Conv2d-130          [-1, 1, 224, 224]              33\n",
      "          Conv2d-131          [-1, 1, 224, 224]              33\n",
      "================================================================\n",
      "Total params: 9,182,052\n",
      "Trainable params: 9,182,052\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 958.95\n",
      "Params size (MB): 35.03\n",
      "Estimated Total Size (MB): 994.55\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = UNetpp()\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBackbone_B(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetBackbone_B, self).__init__()\n",
    "\n",
    "        resnet = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "        \n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        self.res_up1 = double_conv(2048,1024)\n",
    "        self.res_up2 = double_conv(1024,512)\n",
    "        self.res_up3 = double_conv(512,256)\n",
    "        self.res_up4 = double_conv(256,128)\n",
    "        self.res_up5 = double_conv(128,64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)     #2048.7.7\n",
    "        features = self.upsample(features)  # 2048.14.14\n",
    "        features = self.res_up1(features) # 1024.14.14\n",
    "        features = self.upsample(features)  # 1024.28.28\n",
    "        features = self.res_up2(features) #512.28.28\n",
    "        features = self.upsample(features)#512.56.56\n",
    "        features = self.res_up3(features)#256.56.56\n",
    "        features = self.upsample(features)#256.112.112\n",
    "        features = self.res_up4(features)#128.112.112\n",
    "        features = self.upsample(features)#128.224.224\n",
    "        features = self.res_up5(features)#64.224.224\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet_UNetpp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Resnet_UNetpp, self).__init__()\n",
    "        self.backbone = ResNetBackbone_B()\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.dconv_down = double_conv(64, 32)\n",
    "        \n",
    "        # ~ 0\n",
    "        self.dconv_down0_0 = double_conv(32, 32)\n",
    "        self.dconv_down1_0 = double_conv(32, 64)\n",
    "        self.dconv_down2_0 = double_conv(64, 128)\n",
    "        self.dconv_down3_0 = double_conv(128, 256)\n",
    "        self.dconv_down4_0 = double_conv(256, 512)\n",
    "\n",
    "        # ~ 1\n",
    "        self.dconv_down0_1 = double_conv(32+64, 32)\n",
    "        self.dconv_down1_1 = double_conv(64+128, 64)\n",
    "        self.dconv_down2_1 = double_conv(128+256, 128)\n",
    "        self.dconv_down3_1 = double_conv(256+512, 256)\n",
    "\n",
    "        #~ 2\n",
    "        self.dconv_down0_2 = double_conv(64+64, 32)\n",
    "        self.dconv_down1_2 = double_conv(128+128, 64)\n",
    "        self.dconv_down2_2 = double_conv(256+256, 128)\n",
    "\n",
    "        #~ 3\n",
    "        self.dconv_down0_3 = double_conv(96+64, 32)\n",
    "        self.dconv_down1_3 = double_conv(192+128, 64)\n",
    "\n",
    "        #~ 4\n",
    "        self.dconv_down0_4 = double_conv(128+64,32)\n",
    "        \n",
    "        self.output1 = nn.Conv2d(32, 1, 1)\n",
    "        self.output2 = nn.Conv2d(32, 1, 1)\n",
    "        self.output3 = nn.Conv2d(32, 1, 1)\n",
    "        self.output4 = nn.Conv2d(32, 1, 1)\n",
    "\n",
    "        self.Drop_out = nn.Dropout2d(0.2) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x) #64,224,224\n",
    "\n",
    "        x = self.dconv_down(x)  #32,224,224\n",
    "\n",
    "        x0_0 = self.dconv_down0_0(x)   #32,224,224\n",
    "        x = self.maxpool(x0_0)          #32,112,112\n",
    "        x1_0 = self.dconv_down1_0(x)     #64,112,112\n",
    "        x = self.upsample(x1_0)\n",
    "        x = torch.cat([x0_0, self.upsample(x1_0)], dim=1)  #64+32,224,224\n",
    "        x0_1 = self.dconv_down0_1(x)   #32,224,224\n",
    "\n",
    "        x = self.maxpool(x1_0)  #64,56,56\n",
    "        x2_0 = self.dconv_down2_0(x)   #128,56,56\n",
    "        x = torch.cat([x1_0,self.upsample(x2_0)],dim=1)  #64+128,112,112\n",
    "        x1_1 = self.dconv_down1_1(x)  #64,112,112\n",
    "        x = torch.cat([x0_0,x0_1,self.upsample(x1_1)], dim=1) #32+32+64,224,224\n",
    "        x0_2 = self.dconv_down0_2(x)  #32,224,224\n",
    "        \n",
    "        x = self.maxpool(x2_0)\n",
    "        x3_0 = self.dconv_down3_0(x)\n",
    "        x = torch.cat([x2_0,self.upsample(x3_0)], dim=1)\n",
    "        x2_1 = self.dconv_down2_1(x)\n",
    "        x = torch.cat([x1_0,x1_1,self.upsample(x2_1)], dim=1)\n",
    "        x1_2 = self.dconv_down1_2(x)\n",
    "        x = torch.cat([x0_0, x0_1, x0_2, self.upsample(x1_2)], dim=1)\n",
    "        x0_3 = self.dconv_down0_3(x)\n",
    "\n",
    "        x = self.maxpool(x3_0)\n",
    "        x4_0 = self.dconv_down4_0(x)\n",
    "        x = torch.cat([x3_0,self.upsample(x4_0)], dim=1)\n",
    "        x3_1 = self.dconv_down3_1(x)\n",
    "        x = torch.cat([x2_0,x2_1,self.upsample(x3_1)], dim=1)\n",
    "        x2_2 = self.dconv_down2_2(x)\n",
    "        x = torch.cat([x1_0, x1_1, x1_2, self.upsample(x2_2)], dim=1)\n",
    "        x1_3 = self.dconv_down1_3(x)\n",
    "        x = torch.cat([x0_0, x0_1, x0_2,x0_3 ,self.upsample(x1_3)], dim=1)\n",
    "        x0_4 = self.dconv_down0_4(x)\n",
    "        \n",
    "        output1 = self.output1(x0_1)\n",
    "        output2 = self.output1(x0_2)\n",
    "        output3 = self.output1(x0_3)\n",
    "        output4 = self.output1(x0_4)\n",
    "\n",
    "        output = (output1 + output2 + output3 + output4)/4\n",
    "   \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "        Upsample-173         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-174         [-1, 1024, 14, 14]      18,875,392\n",
      "     BatchNorm2d-175         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-176         [-1, 1024, 14, 14]               0\n",
      "       Dropout2d-177         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-178         [-1, 1024, 14, 14]       9,438,208\n",
      "     BatchNorm2d-179         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-180         [-1, 1024, 14, 14]               0\n",
      "        Upsample-181         [-1, 1024, 28, 28]               0\n",
      "          Conv2d-182          [-1, 512, 28, 28]       4,719,104\n",
      "     BatchNorm2d-183          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-184          [-1, 512, 28, 28]               0\n",
      "       Dropout2d-185          [-1, 512, 28, 28]               0\n",
      "          Conv2d-186          [-1, 512, 28, 28]       2,359,808\n",
      "     BatchNorm2d-187          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-188          [-1, 512, 28, 28]               0\n",
      "        Upsample-189          [-1, 512, 56, 56]               0\n",
      "          Conv2d-190          [-1, 256, 56, 56]       1,179,904\n",
      "     BatchNorm2d-191          [-1, 256, 56, 56]             512\n",
      "            ReLU-192          [-1, 256, 56, 56]               0\n",
      "       Dropout2d-193          [-1, 256, 56, 56]               0\n",
      "          Conv2d-194          [-1, 256, 56, 56]         590,080\n",
      "     BatchNorm2d-195          [-1, 256, 56, 56]             512\n",
      "            ReLU-196          [-1, 256, 56, 56]               0\n",
      "        Upsample-197        [-1, 256, 112, 112]               0\n",
      "          Conv2d-198        [-1, 128, 112, 112]         295,040\n",
      "     BatchNorm2d-199        [-1, 128, 112, 112]             256\n",
      "            ReLU-200        [-1, 128, 112, 112]               0\n",
      "       Dropout2d-201        [-1, 128, 112, 112]               0\n",
      "          Conv2d-202        [-1, 128, 112, 112]         147,584\n",
      "     BatchNorm2d-203        [-1, 128, 112, 112]             256\n",
      "            ReLU-204        [-1, 128, 112, 112]               0\n",
      "        Upsample-205        [-1, 128, 224, 224]               0\n",
      "          Conv2d-206         [-1, 64, 224, 224]          73,792\n",
      "     BatchNorm2d-207         [-1, 64, 224, 224]             128\n",
      "            ReLU-208         [-1, 64, 224, 224]               0\n",
      "       Dropout2d-209         [-1, 64, 224, 224]               0\n",
      "          Conv2d-210         [-1, 64, 224, 224]          36,928\n",
      "     BatchNorm2d-211         [-1, 64, 224, 224]             128\n",
      "            ReLU-212         [-1, 64, 224, 224]               0\n",
      "ResNetBackbone_B-213         [-1, 64, 224, 224]               0\n",
      "          Conv2d-214         [-1, 32, 224, 224]          18,464\n",
      "     BatchNorm2d-215         [-1, 32, 224, 224]              64\n",
      "            ReLU-216         [-1, 32, 224, 224]               0\n",
      "       Dropout2d-217         [-1, 32, 224, 224]               0\n",
      "          Conv2d-218         [-1, 32, 224, 224]           9,248\n",
      "     BatchNorm2d-219         [-1, 32, 224, 224]              64\n",
      "            ReLU-220         [-1, 32, 224, 224]               0\n",
      "          Conv2d-221         [-1, 32, 224, 224]           9,248\n",
      "     BatchNorm2d-222         [-1, 32, 224, 224]              64\n",
      "            ReLU-223         [-1, 32, 224, 224]               0\n",
      "       Dropout2d-224         [-1, 32, 224, 224]               0\n",
      "          Conv2d-225         [-1, 32, 224, 224]           9,248\n",
      "     BatchNorm2d-226         [-1, 32, 224, 224]              64\n",
      "            ReLU-227         [-1, 32, 224, 224]               0\n",
      "       MaxPool2d-228         [-1, 32, 112, 112]               0\n",
      "          Conv2d-229         [-1, 64, 112, 112]          18,496\n",
      "     BatchNorm2d-230         [-1, 64, 112, 112]             128\n",
      "            ReLU-231         [-1, 64, 112, 112]               0\n",
      "       Dropout2d-232         [-1, 64, 112, 112]               0\n",
      "          Conv2d-233         [-1, 64, 112, 112]          36,928\n",
      "     BatchNorm2d-234         [-1, 64, 112, 112]             128\n",
      "            ReLU-235         [-1, 64, 112, 112]               0\n",
      "        Upsample-236         [-1, 64, 224, 224]               0\n",
      "        Upsample-237         [-1, 64, 224, 224]               0\n",
      "          Conv2d-238         [-1, 32, 224, 224]          27,680\n",
      "     BatchNorm2d-239         [-1, 32, 224, 224]              64\n",
      "            ReLU-240         [-1, 32, 224, 224]               0\n",
      "       Dropout2d-241         [-1, 32, 224, 224]               0\n",
      "          Conv2d-242         [-1, 32, 224, 224]           9,248\n",
      "     BatchNorm2d-243         [-1, 32, 224, 224]              64\n",
      "            ReLU-244         [-1, 32, 224, 224]               0\n",
      "       MaxPool2d-245           [-1, 64, 56, 56]               0\n",
      "          Conv2d-246          [-1, 128, 56, 56]          73,856\n",
      "     BatchNorm2d-247          [-1, 128, 56, 56]             256\n",
      "            ReLU-248          [-1, 128, 56, 56]               0\n",
      "       Dropout2d-249          [-1, 128, 56, 56]               0\n",
      "          Conv2d-250          [-1, 128, 56, 56]         147,584\n",
      "     BatchNorm2d-251          [-1, 128, 56, 56]             256\n",
      "            ReLU-252          [-1, 128, 56, 56]               0\n",
      "        Upsample-253        [-1, 128, 112, 112]               0\n",
      "          Conv2d-254         [-1, 64, 112, 112]         110,656\n",
      "     BatchNorm2d-255         [-1, 64, 112, 112]             128\n",
      "            ReLU-256         [-1, 64, 112, 112]               0\n",
      "       Dropout2d-257         [-1, 64, 112, 112]               0\n",
      "          Conv2d-258         [-1, 64, 112, 112]          36,928\n",
      "     BatchNorm2d-259         [-1, 64, 112, 112]             128\n",
      "            ReLU-260         [-1, 64, 112, 112]               0\n",
      "        Upsample-261         [-1, 64, 224, 224]               0\n",
      "          Conv2d-262         [-1, 32, 224, 224]          36,896\n",
      "     BatchNorm2d-263         [-1, 32, 224, 224]              64\n",
      "            ReLU-264         [-1, 32, 224, 224]               0\n",
      "       Dropout2d-265         [-1, 32, 224, 224]               0\n",
      "          Conv2d-266         [-1, 32, 224, 224]           9,248\n",
      "     BatchNorm2d-267         [-1, 32, 224, 224]              64\n",
      "            ReLU-268         [-1, 32, 224, 224]               0\n",
      "       MaxPool2d-269          [-1, 128, 28, 28]               0\n",
      "          Conv2d-270          [-1, 256, 28, 28]         295,168\n",
      "     BatchNorm2d-271          [-1, 256, 28, 28]             512\n",
      "            ReLU-272          [-1, 256, 28, 28]               0\n",
      "       Dropout2d-273          [-1, 256, 28, 28]               0\n",
      "          Conv2d-274          [-1, 256, 28, 28]         590,080\n",
      "     BatchNorm2d-275          [-1, 256, 28, 28]             512\n",
      "            ReLU-276          [-1, 256, 28, 28]               0\n",
      "        Upsample-277          [-1, 256, 56, 56]               0\n",
      "          Conv2d-278          [-1, 128, 56, 56]         442,496\n",
      "     BatchNorm2d-279          [-1, 128, 56, 56]             256\n",
      "            ReLU-280          [-1, 128, 56, 56]               0\n",
      "       Dropout2d-281          [-1, 128, 56, 56]               0\n",
      "          Conv2d-282          [-1, 128, 56, 56]         147,584\n",
      "     BatchNorm2d-283          [-1, 128, 56, 56]             256\n",
      "            ReLU-284          [-1, 128, 56, 56]               0\n",
      "        Upsample-285        [-1, 128, 112, 112]               0\n",
      "          Conv2d-286         [-1, 64, 112, 112]         147,520\n",
      "     BatchNorm2d-287         [-1, 64, 112, 112]             128\n",
      "            ReLU-288         [-1, 64, 112, 112]               0\n",
      "       Dropout2d-289         [-1, 64, 112, 112]               0\n",
      "          Conv2d-290         [-1, 64, 112, 112]          36,928\n",
      "     BatchNorm2d-291         [-1, 64, 112, 112]             128\n",
      "            ReLU-292         [-1, 64, 112, 112]               0\n",
      "        Upsample-293         [-1, 64, 224, 224]               0\n",
      "          Conv2d-294         [-1, 32, 224, 224]          46,112\n",
      "     BatchNorm2d-295         [-1, 32, 224, 224]              64\n",
      "            ReLU-296         [-1, 32, 224, 224]               0\n",
      "       Dropout2d-297         [-1, 32, 224, 224]               0\n",
      "          Conv2d-298         [-1, 32, 224, 224]           9,248\n",
      "     BatchNorm2d-299         [-1, 32, 224, 224]              64\n",
      "            ReLU-300         [-1, 32, 224, 224]               0\n",
      "       MaxPool2d-301          [-1, 256, 14, 14]               0\n",
      "          Conv2d-302          [-1, 512, 14, 14]       1,180,160\n",
      "     BatchNorm2d-303          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-304          [-1, 512, 14, 14]               0\n",
      "       Dropout2d-305          [-1, 512, 14, 14]               0\n",
      "          Conv2d-306          [-1, 512, 14, 14]       2,359,808\n",
      "     BatchNorm2d-307          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-308          [-1, 512, 14, 14]               0\n",
      "        Upsample-309          [-1, 512, 28, 28]               0\n",
      "          Conv2d-310          [-1, 256, 28, 28]       1,769,728\n",
      "     BatchNorm2d-311          [-1, 256, 28, 28]             512\n",
      "            ReLU-312          [-1, 256, 28, 28]               0\n",
      "       Dropout2d-313          [-1, 256, 28, 28]               0\n",
      "          Conv2d-314          [-1, 256, 28, 28]         590,080\n",
      "     BatchNorm2d-315          [-1, 256, 28, 28]             512\n",
      "            ReLU-316          [-1, 256, 28, 28]               0\n",
      "        Upsample-317          [-1, 256, 56, 56]               0\n",
      "          Conv2d-318          [-1, 128, 56, 56]         589,952\n",
      "     BatchNorm2d-319          [-1, 128, 56, 56]             256\n",
      "            ReLU-320          [-1, 128, 56, 56]               0\n",
      "       Dropout2d-321          [-1, 128, 56, 56]               0\n",
      "          Conv2d-322          [-1, 128, 56, 56]         147,584\n",
      "     BatchNorm2d-323          [-1, 128, 56, 56]             256\n",
      "            ReLU-324          [-1, 128, 56, 56]               0\n",
      "        Upsample-325        [-1, 128, 112, 112]               0\n",
      "          Conv2d-326         [-1, 64, 112, 112]         184,384\n",
      "     BatchNorm2d-327         [-1, 64, 112, 112]             128\n",
      "            ReLU-328         [-1, 64, 112, 112]               0\n",
      "       Dropout2d-329         [-1, 64, 112, 112]               0\n",
      "          Conv2d-330         [-1, 64, 112, 112]          36,928\n",
      "     BatchNorm2d-331         [-1, 64, 112, 112]             128\n",
      "            ReLU-332         [-1, 64, 112, 112]               0\n",
      "        Upsample-333         [-1, 64, 224, 224]               0\n",
      "          Conv2d-334         [-1, 32, 224, 224]          55,328\n",
      "     BatchNorm2d-335         [-1, 32, 224, 224]              64\n",
      "            ReLU-336         [-1, 32, 224, 224]               0\n",
      "       Dropout2d-337         [-1, 32, 224, 224]               0\n",
      "          Conv2d-338         [-1, 32, 224, 224]           9,248\n",
      "     BatchNorm2d-339         [-1, 32, 224, 224]              64\n",
      "            ReLU-340         [-1, 32, 224, 224]               0\n",
      "          Conv2d-341          [-1, 1, 224, 224]              33\n",
      "          Conv2d-342          [-1, 1, 224, 224]              33\n",
      "          Conv2d-343          [-1, 1, 224, 224]              33\n",
      "          Conv2d-344          [-1, 1, 224, 224]              33\n",
      "================================================================\n",
      "Total params: 70,431,428\n",
      "Trainable params: 70,431,428\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 1697.20\n",
      "Params size (MB): 268.67\n",
      "Estimated Total Size (MB): 1966.45\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = Resnet_UNetpp()\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
