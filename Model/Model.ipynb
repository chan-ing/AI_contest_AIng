{"cells":[{"cell_type":"markdown","id":"d73d24e3-5c9e-4ade-9e6e-ca6f46a2d914","metadata":{"id":"d73d24e3-5c9e-4ade-9e6e-ca6f46a2d914"},"source":["## Import"]},{"cell_type":"code","execution_count":1,"id":"f3jm5Pz7nXrl","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16798,"status":"ok","timestamp":1689277532689,"user":{"displayName":"민서","userId":"00695914076813373462"},"user_tz":-540},"id":"f3jm5Pz7nXrl","outputId":"83ae97f2-29f1-45fd-99b7-8c7180852b5d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"id":"ad9b681e-370a-4cfa-a452-dd2d7f0cd77f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21623,"status":"ok","timestamp":1689277557277,"user":{"displayName":"민서","userId":"00695914076813373462"},"user_tz":-540},"id":"ad9b681e-370a-4cfa-a452-dd2d7f0cd77f","outputId":"0be3ae4e-382c-4f94-c279-1e0c1b18c466"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting efficientnet_pytorch\n","  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet_pytorch) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->efficientnet_pytorch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->efficientnet_pytorch) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet_pytorch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\n","Building wheels for collected packages: efficientnet_pytorch\n","  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16427 sha256=7aa849808ce8c02790f885d51f55f23c525676593dc391c426baa52eedf4ebe4\n","  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n","Successfully built efficientnet_pytorch\n","Installing collected packages: efficientnet_pytorch\n","Successfully installed efficientnet_pytorch-0.7.1\n","Collecting segmentation_models_pytorch\n","  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.15.2+cu118)\n","Collecting pretrainedmodels==0.7.4 (from segmentation_models_pytorch)\n","  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: efficientnet-pytorch==0.7.1 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.7.1)\n","Collecting timm==0.9.2 (from segmentation_models_pytorch)\n","  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (4.65.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (8.4.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.0.1+cu118)\n","Collecting munch (from pretrainedmodels==0.7.4->segmentation_models_pytorch)\n","  Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0)\n","Collecting huggingface-hub (from timm==0.9.2->segmentation_models_pytorch)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors (from timm==0.9.2->segmentation_models_pytorch)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.27.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (16.0.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (2023.6.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (23.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\n","Building wheels for collected packages: pretrainedmodels\n","  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=b3c491c3a583cdabf8872d2810595dec6e7a6ce29e692b1db133ee35047b7a4b\n","  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n","Successfully built pretrainedmodels\n","Installing collected packages: safetensors, munch, huggingface-hub, timm, pretrainedmodels, segmentation_models_pytorch\n","Successfully installed huggingface-hub-0.16.4 munch-4.0.0 pretrainedmodels-0.7.4 safetensors-0.3.1 segmentation_models_pytorch-0.3.3 timm-0.9.2\n"]}],"source":["import os\n","import cv2\n","import pandas as pd\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from torchsummary import summary\n","import torchvision.models as models\n","!pip install efficientnet_pytorch\n","from efficientnet_pytorch import EfficientNet\n","import torch.nn.functional as F\n","!pip install segmentation_models_pytorch\n","import segmentation_models_pytorch as smp\n","\n","from tqdm import tqdm\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":12,"id":"rsUQB4gNFGmB","metadata":{"executionInfo":{"elapsed":415,"status":"ok","timestamp":1689277800469,"user":{"displayName":"민서","userId":"00695914076813373462"},"user_tz":-540},"id":"rsUQB4gNFGmB"},"outputs":[],"source":["torch.cuda.set_per_process_memory_fraction(0.7)"]},{"cell_type":"code","execution_count":4,"id":"VweMiPZRgZu5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1689277566480,"user":{"displayName":"민서","userId":"00695914076813373462"},"user_tz":-540},"id":"VweMiPZRgZu5","outputId":"0a0d97e5-2e6e-439e-bf52-9ae6c8681c38"},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n"]}],"source":["print(torch.cuda.is_available())"]},{"cell_type":"markdown","id":"20ff3de5-0d0e-497b-ac75-d5179a3f65d3","metadata":{"id":"20ff3de5-0d0e-497b-ac75-d5179a3f65d3"},"source":["## Utils"]},{"cell_type":"code","execution_count":5,"id":"838e1d83-8670-407b-82f6-bf9652f58639","metadata":{"executionInfo":{"elapsed":411,"status":"ok","timestamp":1689277568835,"user":{"displayName":"민서","userId":"00695914076813373462"},"user_tz":-540},"id":"838e1d83-8670-407b-82f6-bf9652f58639"},"outputs":[],"source":["# RLE 디코딩 함수\n","def rle_decode(mask_rle, shape):\n","    s = mask_rle.split()\n","    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n","    starts -= 1\n","    ends = starts + lengths\n","    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n","    for lo, hi in zip(starts, ends):\n","        img[lo:hi] = 1\n","    return img.reshape(shape)\n","\n","# RLE 인코딩 함수\n","def rle_encode(mask):\n","    pixels = mask.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)"]},{"cell_type":"markdown","id":"be76a29e-e9c2-411a-a569-04166f074184","metadata":{"id":"be76a29e-e9c2-411a-a569-04166f074184"},"source":["## Custom Dataset"]},{"cell_type":"code","execution_count":6,"id":"a8496767-2f64-4285-bec4-c6f53a1fd9d2","metadata":{"executionInfo":{"elapsed":312,"status":"ok","timestamp":1689277571192,"user":{"displayName":"민서","userId":"00695914076813373462"},"user_tz":-540},"id":"a8496767-2f64-4285-bec4-c6f53a1fd9d2"},"outputs":[],"source":["class SatelliteDataset(Dataset):\n","    def __init__(self, csv_file, transform=None, infer=False):\n","        self.data = pd.read_csv(csv_file)\n","        self.transform = transform\n","        self.infer = infer\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.data.iloc[idx, 1]   #    img_path = \"./train_img/TRAIN_0000.png\"\n","        img_path = '/content/drive/MyDrive/Colab_Notebooks' + img_path.lstrip(\".\")\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        if self.infer:\n","            if self.transform:\n","                image = self.transform(image=image)['image']\n","            return image\n","\n","        mask_rle = self.data.iloc[idx, 2]\n","        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n","\n","        if self.transform:\n","            augmented = self.transform(image=image, mask=mask)\n","            image = augmented['image']\n","            mask = augmented['mask']\n","\n","        return image, mask"]},{"cell_type":"markdown","id":"dc955893-22fd-4320-88be-7aa0d790cbd9","metadata":{"id":"dc955893-22fd-4320-88be-7aa0d790cbd9"},"source":["## Data Loader"]},{"cell_type":"code","execution_count":10,"id":"1b708503-2ff9-4584-9d73-40990b3572f8","metadata":{"executionInfo":{"elapsed":2335,"status":"ok","timestamp":1689277729104,"user":{"displayName":"민서","userId":"00695914076813373462"},"user_tz":-540},"id":"1b708503-2ff9-4584-9d73-40990b3572f8"},"outputs":[],"source":["torch.multiprocessing.freeze_support()\n","transform = A.Compose(\n","    [\n","        A.Resize(224, 224),\n","        A.Normalize(),\n","        ToTensorV2()\n","    ]\n",")\n","\n","dataset = SatelliteDataset(csv_file='/content/drive/MyDrive/Colab_Notebooks/train.csv', transform=transform)\n","dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)"]},{"cell_type":"markdown","id":"f42501fc-b573-4893-a7c4-5e280dfdaf09","metadata":{"id":"f42501fc-b573-4893-a7c4-5e280dfdaf09"},"source":["## Define Model"]},{"cell_type":"code","execution_count":8,"id":"65960bfb-803a-4c40-b713-6f647779e4ea","metadata":{"executionInfo":{"elapsed":413,"status":"ok","timestamp":1689277581634,"user":{"displayName":"민서","userId":"00695914076813373462"},"user_tz":-540},"id":"65960bfb-803a-4c40-b713-6f647779e4ea"},"outputs":[],"source":["# U-Net의 기본 구성 요소인 Double Convolution Block을 정의합니다.\n","def double_conv(in_channels, out_channels):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True),\n","        nn.Dropout2d(p=0.2),\n","        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True)\n","    )\n","\n","class ResNetBackbone(nn.Module):\n","    def __init__(self):\n","        super(ResNetBackbone, self).__init__()\n","\n","        resnet = models.resnet50(pretrained=True)\n","\n","        # ResNet의 마지막 두 레이어를 제거하여 feature map을 얻습니다.\n","        self.features = nn.Sequential(*list(resnet.children())[:-2])\n","\n","        self.upsample = nn.Upsample(size=(224, 224), mode='bilinear', align_corners=True)\n","        self.res_down1 = double_conv(2048,64)\n","\n","    def forward(self, x):\n","        features = self.features(x)\n","        features = self.upsample(features)\n","        features = self.res_down1(features)\n","        return features\n","\n","class UNet(nn.Module):\n","    def __init__(self):\n","        super(UNet, self).__init__()\n","        self.backbone = ResNetBackbone()\n","\n","        self.dconv_down1 = double_conv(64, 64)\n","        self.dconv_down2 = double_conv(64, 128)\n","        self.dconv_down3 = double_conv(128, 256)\n","        self.dconv_down4 = double_conv(256, 512)\n","\n","        self.maxpool = nn.MaxPool2d(2)\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","\n","        self.dconv_up3 = double_conv(256 + 512, 256)\n","        self.dconv_up2 = double_conv(128 + 256, 128)\n","        self.dconv_up1 = double_conv(128 + 64, 64)\n","\n","        self.conv_last = nn.Conv2d(64, 1, 1)\n","\n","    def forward(self, x):\n","        x = self.backbone(x)   # 3 -> 64\n","\n","        conv1 = self.dconv_down1(x)   # 64 -> 64\n","        x = self.maxpool(conv1)\n","\n","        conv2 = self.dconv_down2(x)  # 64 -> 128\n","        x = self.maxpool(conv2)\n","\n","        conv3 = self.dconv_down3(x)  # 128 -> 256\n","        x = self.maxpool(conv3)\n","\n","        x = self.dconv_down4(x)  # 256 -> 512\n","\n","        x = self.upsample(x)\n","        x = torch.cat([x, conv3], dim=1)\n","\n","        x = self.dconv_up3(x)\n","        x = self.upsample(x)\n","        x = torch.cat([x, conv2], dim=1)\n","\n","        x = self.dconv_up2(x)\n","        x = self.upsample(x)\n","        x = torch.cat([x, conv1], dim=1)\n","\n","        x = self.dconv_up1(x)\n","\n","        out = self.conv_last(x)\n","\n","        return out\n","\n","class EfficientNetBackbone(nn.Module):\n","    def __init__(self, pretrained=True):\n","        super(EfficientNetBackbone, self).__init__()\n","        self.model = EfficientNet.from_pretrained('efficientnet-b0') if pretrained else EfficientNet.from_name('efficientnet-b0')\n","\n","    def forward(self, x):\n","        features = self.model.extract_features(x)\n","        return features\n","\n","\n","class eff_UNet(nn.Module):\n","    def __init__(self):\n","        super(eff_UNet, self).__init__()\n","        self.backbone = EfficientNetBackbone(True)\n","\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","\n","        self.dconv_up4 = double_conv(1280, 512)\n","        self.dconv_up3 = double_conv(512, 256)\n","        self.dconv_up2 = double_conv(256, 128)\n","        self.dconv_up1 = double_conv(128, 64)\n","\n","        self.conv_last = nn.Conv2d(64, 1, 1)\n","\n","        self.Drop_out = nn.Dropout2d(0.2)\n","\n","    def forward(self, x):\n","        x = self.backbone(x)\n","\n","        x = self.upsample(x)   #1280,14,14\n","        x = self.dconv_up4(x)  #512,14,14\n","\n","        x = self.upsample(x)  #512,28,28\n","        x = self.dconv_up3(x) #256,28,28\n","\n","        x = self.upsample(x) #256,56,56\n","        x = self.dconv_up2(x) #128,56,56\n","\n","        x = self.upsample(x) #128,112,112\n","        x = self.dconv_up1(x) #64,112,112\n","\n","        x = self.upsample(x) #64,224,224\n","        out = self.conv_last(x)\n","\n","        return out\n","\n","class basic_UNet(nn.Module):\n","    def __init__(self):\n","        super(basic_UNet, self).__init__()\n","        self.dconv_down1 = double_conv(3, 64)\n","        self.dconv_down2 = double_conv(64, 128)\n","        self.dconv_down3 = double_conv(128, 256)\n","        self.dconv_down4 = double_conv(256, 512)\n","\n","        self.maxpool = nn.MaxPool2d(2)\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","\n","        self.dconv_up3 = double_conv(256 + 512, 256)\n","        self.dconv_up2 = double_conv(128 + 256, 128)\n","        self.dconv_up1 = double_conv(128 + 64, 64)\n","\n","        self.conv_last = nn.Conv2d(64, 1, 1)\n","\n","    def forward(self, x):\n","        conv1 = self.dconv_down1(x)\n","        x = self.maxpool(conv1)\n","\n","        conv2 = self.dconv_down2(x)\n","        x = self.maxpool(conv2)\n","\n","        conv3 = self.dconv_down3(x)\n","        x = self.maxpool(conv3)\n","\n","        x = self.dconv_down4(x)\n","\n","        x = self.upsample(x)\n","        x = torch.cat([x, conv3], dim=1)\n","\n","        x = self.dconv_up3(x)\n","        x = self.upsample(x)\n","        x = torch.cat([x, conv2], dim=1)\n","\n","        x = self.dconv_up2(x)\n","        x = self.upsample(x)\n","        x = torch.cat([x, conv1], dim=1)\n","\n","        x = self.dconv_up1(x)\n","\n","        out = self.conv_last(x)\n","\n","        return out\n","\n","class UNetpp(nn.Module):\n","    def __init__(self):\n","        super(UNetpp, self).__init__()\n","\n","\n","        self.maxpool = nn.MaxPool2d(2)\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","\n","        self.dconv_down = double_conv(3, 32)\n","        # ~ 0\n","        self.dconv_down0_0 = double_conv(32, 32)\n","        self.dconv_down1_0 = double_conv(32, 64)\n","        self.dconv_down2_0 = double_conv(64, 128)\n","        self.dconv_down3_0 = double_conv(128, 256)\n","        self.dconv_down4_0 = double_conv(256, 512)\n","\n","        # ~ 1\n","        self.dconv_down0_1 = double_conv(32+64, 32)\n","        self.dconv_down1_1 = double_conv(64+128, 64)\n","        self.dconv_down2_1 = double_conv(128+256, 128)\n","        self.dconv_down3_1 = double_conv(256+512, 256)\n","\n","        #~ 2\n","        self.dconv_down0_2 = double_conv(64+64, 32)\n","        self.dconv_down1_2 = double_conv(128+128, 64)\n","        self.dconv_down2_2 = double_conv(256+256, 128)\n","\n","        #~ 3\n","        self.dconv_down0_3 = double_conv(96+64, 32)\n","        self.dconv_down1_3 = double_conv(192+128, 64)\n","\n","        #~ 4\n","        self.dconv_down0_4 = double_conv(128+64,32)\n","\n","\n","        self.output1 = nn.Conv2d(32, 1, 1)\n","        self.output2 = nn.Conv2d(32, 1, 1)\n","        self.output3 = nn.Conv2d(32, 1, 1)\n","        self.output4 = nn.Conv2d(32, 1, 1)\n","\n","        self.Drop_out = nn.Dropout2d(0.2)\n","\n","    def forward(self, x):\n","        x = self.dconv_down(x)  #32,224,224\n","\n","        x0_0 = self.dconv_down0_0(x)   #32,224,224\n","        x = self.maxpool(x0_0)           #32,112,112\n","        x1_0 = self.dconv_down1_0(x)       #64,112,112\n","        x = self.upsample(x1_0)\n","        x = torch.cat([x0_0, self.upsample(x1_0)], dim=1)  #64+32,224,224\n","        x0_1 = self.dconv_down0_1(x)   #32,224,224\n","\n","        x = self.maxpool(x1_0)  #64,56,56\n","        x2_0 = self.dconv_down2_0(x)   #128,56,56\n","        x = torch.cat([x1_0,self.upsample(x2_0)],dim=1)  #64+128,112,112\n","        x1_1 = self.dconv_down1_1(x)  #64,112,112\n","        x = torch.cat([x0_0,x0_1,self.upsample(x1_1)], dim=1) #32+32+64,224,224\n","        x0_2 = self.dconv_down0_2(x)  #32,224,224\n","\n","        x = self.maxpool(x2_0)\n","        x3_0 = self.dconv_down3_0(x)\n","        x = torch.cat([x2_0,self.upsample(x3_0)], dim=1)\n","        x2_1 = self.dconv_down2_1(x)\n","        x = torch.cat([x1_0,x1_1,self.upsample(x2_1)], dim=1)\n","        x1_2 = self.dconv_down1_2(x)\n","        x = torch.cat([x0_0, x0_1, x0_2, self.upsample(x1_2)], dim=1)\n","        x0_3 = self.dconv_down0_3(x)\n","\n","        x = self.maxpool(x3_0)\n","        x4_0 = self.dconv_down4_0(x)\n","        x = torch.cat([x3_0,self.upsample(x4_0)], dim=1)\n","        x3_1 = self.dconv_down3_1(x)\n","        x = torch.cat([x2_0,x2_1,self.upsample(x3_1)], dim=1)\n","        x2_2 = self.dconv_down2_2(x)\n","        x = torch.cat([x1_0, x1_1, x1_2, self.upsample(x2_2)], dim=1)\n","        x1_3 = self.dconv_down1_3(x)\n","        x = torch.cat([x0_0, x0_1, x0_2,x0_3 ,self.upsample(x1_3)], dim=1)\n","        x0_4 = self.dconv_down0_4(x)\n","\n","\n","        output1 = self.output1(x0_1)\n","        output2 = self.output1(x0_2)\n","        output3 = self.output1(x0_3)\n","        output4 = self.output1(x0_4)\n","\n","        output = (output1 + output2 + output3 + output4)/4\n","\n","        return output\n","\n","\n","class EnsembleModel(nn.Module):\n","    def __init__(self, model1, model2,model3,model4):\n","        super(EnsembleModel, self).__init__()\n","        self.model1 = model1\n","        self.model2 = model2\n","        self.model3 = model3\n","        self.model4 = model4\n","\n","    def forward(self, x):\n","        output1 = self.model1(x)\n","        output2 = self.model2(x)\n","        output3 = self.model3(x)\n","        output4 = self.model4(x)\n","        ensemble_output = (output1 + output2 + output3+ output4) / 4  # 두 모델의 예측을 평균합니다.\n","        return ensemble_output\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, alpha=0.5, gamma=2):\n","        super(FocalLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, inputs, targets):\n","        bce_loss = nn.BCEWithLogitsLoss(reduction='none')(inputs, targets)\n","\n","        pt = torch.exp(-bce_loss)  # 확률 값 계산\n","        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n","\n","        return focal_loss.mean()\n","\n","# def lovasz_hinge(logits, targets):\n","#     signs = 2 * targets - 1\n","#     errors = (1 - logits * signs)\n","#     errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n","#     perm = perm.squeeze()\n","#     gt_sorted = targets[perm]\n","#     grad = lovasz_grad(gt_sorted)\n","\n","#     loss = torch.dot(F.relu(errors_sorted), grad)\n","#     return loss\n","\n","# def lovasz_grad(gt_sorted):\n","#     gts = gt_sorted.float()\n","#     intersection = gts.sum()\n","#     union = gts.numel()\n","#     grad = torch.zeros_like(gts)\n","\n","#     for i in range(1, len(gts)):\n","#         grad[i] = (intersection - gts[:i].sum()) / (union - gts[:i].numel())\n","\n","#     return grad\n","\n","# # 예시 사용법\n","# logits = torch.tensor([0.3, -0.8, 0.1])  # 모델의 출력\n","# targets = torch.tensor([1, 0, 1])  # 실제 타깃\n","\n","# loss = lovasz_hinge(logits, targets)\n","# print(loss)"]},{"cell_type":"markdown","id":"a0895765-fba0-4fd9-b955-a6c0e43012e9","metadata":{"id":"a0895765-fba0-4fd9-b955-a6c0e43012e9"},"source":["## Model Train"]},{"cell_type":"code","execution_count":null,"id":"63efb381-98c6-4d9b-a3b6-bd11c7fa8c41","metadata":{"id":"63efb381-98c6-4d9b-a3b6-bd11c7fa8c41"},"outputs":[],"source":["# model 초기화\n","model1 = UNet().to(device)\n","model2 = basic_UNet().to(device)\n","model3 = eff_UNet().to(device)\n","model4 = UNetpp().to(device)\n","\n","ensemble_model = EnsembleModel(model1, model2,model3,model4).to(device)\n","\n","# loss function과 optimizer 정의\n","criterion = FocalLoss(alpha=0.5, gamma = 2)\n","#criterion = lovasz_hinge\n","optimizer = torch.optim.Adam(ensemble_model.parameters(), lr=0.001)\n","\n","# training loop\n","for epoch in range(5):  # 5 에폭 동안 학습합니다.\n","    ensemble_model.train()\n","    epoch_loss = 0\n","    for images, masks in tqdm(dataloader):\n","        images = images.float().to(device)\n","        masks = masks.float().to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = ensemble_model(images)\n","        loss = criterion(outputs, masks.unsqueeze(1))\n","        #loss = criterion(outputs, masks)\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(dataloader)}')"]},{"cell_type":"markdown","id":"c32eb51c-a3fe-4e11-a616-3a717ba16f7e","metadata":{"id":"c32eb51c-a3fe-4e11-a616-3a717ba16f7e"},"source":["## Inference"]},{"cell_type":"code","execution_count":null,"id":"12371c8b-0c78-47df-89ec-2d8b55c8ea94","metadata":{"id":"12371c8b-0c78-47df-89ec-2d8b55c8ea94"},"outputs":[],"source":["test_dataset = SatelliteDataset(csv_file='./test.csv', transform=transform, infer=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)"]},{"cell_type":"code","execution_count":null,"id":"355b431c-ac8e-4c40-9046-4d53e4bab14a","metadata":{"id":"355b431c-ac8e-4c40-9046-4d53e4bab14a","outputId":"efbf6434-73d0-4294-ec7e-551bf92836c6"},"outputs":[{"name":"stderr","output_type":"stream","text":["100% 3790/3790 [04:18<00:00, 14.65it/s]\n"]}],"source":["with torch.no_grad():\n","    ensemble_model.eval()\n","    result = []\n","    for images in tqdm(test_dataloader):\n","        images = images.float().to(device)\n","\n","        outputs = ensemble_model(images)\n","        masks = torch.sigmoid(outputs).cpu().numpy()\n","        masks = np.squeeze(masks, axis=1)\n","        masks = (masks > 0.35).astype(np.uint8) # Threshold = 0.35\n","\n","        for i in range(len(images)):\n","            mask_rle = rle_encode(masks[i])\n","            if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n","                result.append(-1)\n","            else:\n","                result.append(mask_rle)"]},{"cell_type":"markdown","id":"36c2cbbb-04f1-4f9c-b4df-4b744dfce046","metadata":{"id":"36c2cbbb-04f1-4f9c-b4df-4b744dfce046"},"source":["## Submission"]},{"cell_type":"code","execution_count":null,"id":"f6543d00-32b3-4f2d-a572-d0879fd0a497","metadata":{"id":"f6543d00-32b3-4f2d-a572-d0879fd0a497"},"outputs":[],"source":["submit = pd.read_csv('./sample_submission.csv')\n","submit['mask_rle'] = result"]},{"cell_type":"code","execution_count":null,"id":"da10cb6f-0826-4755-a376-97b695ae8f86","metadata":{"id":"da10cb6f-0826-4755-a376-97b695ae8f86"},"outputs":[],"source":["submit.to_csv('./submit.csv', index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":5}
